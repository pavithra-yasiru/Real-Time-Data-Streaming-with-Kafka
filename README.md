# ğŸ“¡ Real-Time Kafka Producing & Consuming with Confluent Kafka Cloud  
### ğŸ§© Python | Jupyter Notebooks | Confluent Cloud | Real-Time Streaming

This project demonstrates **real-time message streaming** using **Apache Kafka** hosted on **Confluent Kafka Cloud**.  
It includes:

- âœ”ï¸ A Kafka **Producer** notebook (`Kafka Producer.ipynb`)  
- âœ”ï¸ A Kafka **Consumer** notebook (`Kafka Consumer.ipynb`)  
- âœ”ï¸ A dataset used for streaming (`first_100_customers.csv`)  
- âœ”ï¸ A real-time demo video showing produce & consume (`Kafka Producing & Consuming.mp4`)

Perfect for anyone learning real-time event streaming and cloud-based Kafka pipelines. ğŸš€

## ğŸ“ Project Structure
â”œâ”€â”€ Kafka Producer.ipynb
â”œâ”€â”€ Kafka Consumer.ipynb
â”œâ”€â”€ first_100_customers.csv
â””â”€â”€ Kafka Producing & Consuming.mp4 (demo video)

## ğŸ—ï¸ **Project Overview**

This project simulates **real-time customer events** being published and consumed via Kafka on **Confluent Cloud**.

### ğŸ”¹ **Producer Notebook**
The producer:

- Reads the dataset **`first_100_customers.csv`**
- Converts each row to a JSON message  
- Publishes messages to a Confluent Kafka topic  
- Uses delivery callbacks  
- Ensures message acknowledgements

### ğŸ”¹ **Consumer Notebook**
The consumer:

- Connects to the same Kafka topic  
- Continuously polls for messages  
- Decodes and prints real-time streaming events  
- Demonstrates reliable, low-latency consumption  

## ğŸ§ª **Dataset: `first_100_customers.csv`**

The dataset contains customer records used as the streaming source.  
Each row is turned into a Kafka message by the producer.

## ğŸ¥ **Demo (Screen Recording)**

The screen recording **â€œKafka Producing & Consumingâ€** shows:

- Left side â Kafka Producer sending messages  
- Right side â Kafka Consumer receiving them in real-time  

This visually demonstrates the full streaming pipeline working live. âš¡

## â˜ï¸ **Confluent Kafka Cloud Setup (Summary)**

The notebooks assume the following were already configured:

1. A Confluent Kafka Cluster  
2. Created a topic (e.g., `customer-stream`)  
3. API Key & Secret  
4. Bootstrap Server URL  
5. Installed required dependencies:
   ```bash
   pip install confluent-kafka pandas
   Environment variables / configs are loaded inside the notebooks.

## ğŸ› ï¸ Technologies Used

Python
Jupyter Notebook
Confluent Kafka Cloud
Apache Kafka
Pandas

## ğŸ‘¤ Author
Pavithra Yasiru

